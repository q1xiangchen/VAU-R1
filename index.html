<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We propose VAU-R1, a Reinforcement Fine-Tuning (RFT) framework that improves the reasoning ability of MLLMs for video anomaly understanding (VAU).">
  <meta name="keywords" content="VAU-R1, Reinforcement Fine-Tuning, MLLMs, Video Anomaly Understanding, VAU">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body" style="margin-bottom: -1cm;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VAU-R1: Advancing Video Anomaly Understanding via </br> Reinforcement Fine-Tuning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tom-roujiang.github.io/liyun_zhu/">Liyun Zhu</a><sup>1,2</sup>, 
              <a href="https://q1xiangchen.github.io">Qixiang Chen</a><sup>1</sup>, 
              <a href="https://xishen0220.github.io">Xi Shen</a><sup>3</sup>,
              <a href="https://vinthony.github.io/academic/">Xiaodong Cun</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Australian National University</span>&nbsp&nbsp&nbsp&nbsp
            <span class="author-block"><sup>2</sup><a href='https://gvclab.github.io'>GVC Lab, Great Bay University</a></span>&nbsp&nbsp&nbsp&nbsp
            <span class="author-block"><sup>3</sup>Intellindust</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2505.23504"
                   class="external-link button is-normal is-rounded">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/GVCLab/VAU-R1"
                   class="external-link button is-normal is-rounded">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- HuggingFace Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/7xiang/VAU-Bench"
                   class="external-link button is-normal is-rounded">
                  <span class="icon">
                      <img src="./static/images/huggingface_logo-noborder.svg">
                  </span>
                  <span>&nbspData</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="./static/images/pipeline.png" alt="VAU-R1 Teaser Image">
      </figure>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-ten-fifths">
        <!-- <h2 class="title is-3">Overview</h2> -->
        <div class="content has-text-justified">
          <h2>üí° Motivation</h2>
          <p>
            Video Anomaly Understanding (VAU) is vital for real-world applications such as smart cities, security surveillance, and disaster response. Unlike conventional video analysis tasks, VAU demands fine-grained inspection and multi-step reasoning in complex, dynamic, and often unpredictable environments. While early research focused primarily on binary classification (normal vs. abnormal) and temporal localization, such methods offer limited interpretability and provide little understanding of the underlying causes of anomalies.
          </p>
          <p>
            Recent advances in Multimodal Large Language Models (MLLMs) have improved transparency by generating textual descriptions of anomalous events. However, four major challenges remain:
          </p>
          <ol>
            <li>Lack of coherent, multi-step reasoning chains to explain anomalies</li>
            <li>Unclear strategies for decomposing complex VAU tasks into effective subtasks for model supervision</li>
            <li>Absence of rich, annotated benchmarks that support structured causal reasoning</li>
            <li>Insufficient evaluation protocols to assess the quality of visual reasoning</li>
          </ol>
          <p>
            To move beyond shallow detection and enable deeper, interpretable anomaly understanding, we ask a central question:
            <strong>What types of tasks (or reasoning pathways) can help enhance the understanding of anomalies?</strong>
            To explore this, we introduce a framework consisting of three parallel training tasks. These tasks are co-trained either individually or in combination, and their effectiveness is evaluated through a dedicated anomaly analysis task.
          </p>
          <ul>
            <li style="background-color: #FFE7C3;"><strong>Perception</strong>: Identify the scene and relevant entities through guided multiple-choice questions</li>
            <li style="background-color: #D8E9FF;"><strong>Grounding</strong>: Accurately localize the temporal segment where the anomaly occurs</li>
            <li style="background-color: #E9DAF2;"><strong>Classification</strong>: Categorize the anomaly based on observed evidence (e.g., fighting, robbery)</li>
            <li style="background-color: #FDDCDC; list-style-type: square;"><strong>Analysis</strong>: Evaluate how well the model can explain why the anomaly occurred through structured causal reasoning</li>
          </ul>
          <p>
            This decomposition enables models to acquire diverse reasoning skills, build deeper semantic understanding, and generate more interpretable outputs aligned with each task‚Äîultimately supporting more robust and explainable video anomaly analysis.
          </p>




          <h3>üîç Key Contributions</h3>
          <ul>
            <li><strong>VAU-R1:</strong> A data-efficient Reinforcement Fine-Tuning (RFT) framework that enhances the multi-step reasoning capabilities of Multimodal Large Language Models (MLLMs) for video anomaly understanding. It is optimized using task-specific reward signals and consistently outperforms supervised fine-tuning across VAU tasks.</li>
            
            <li><strong>VAU-Bench:</strong> The first benchmark tailored for Chain-of-Thought video anomaly reasoning. It includes 4,600+ videos with diverse anomaly types and provides rich annotations such as multiple-choice QA, temporal intervals, anomaly classes, and step-by-step reasoning rationales.</li>
            
            <li><strong>Unified Evaluation Protocol:</strong> A comprehensive evaluation framework that assesses models along four key dimensions: perception accuracy, temporal grounding precision, classification correctness, and reasoning quality‚Äîcapturing both interpretability and analytical depth.</li>
          </ul>

          <figure class="image">
            <img src="./static/images/bench.png">
            <p style="font-size: medium; text-align: center;"><i>
            <b>Statistics of our VAU-Bench.</b>
            (a) Distribution of main anomaly types.
            (b) Distribution of video durations <i>(top)</i> and the proportion of anomalous segments within each video <i>(bottom)</i>.  
            (c) The evaluation criteria for four VAU tasks.</i>
          </p>
          </figure>
        </div>
      </div>
    </div>
    <!--/ Overview. -->
  </div>
</section>

<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">üìä Results</h2>

    <p>
      We evaluate <strong>VAU-R1</strong> on the <strong>VAU-Bench</strong> dataset, which encompasses diverse real-world scenarios from MSAD, UCF-Crime, and ECVA. The evaluation focuses on four key tasks: multiple-choice question answering (QA), temporal anomaly grounding, anomaly classification, and anomaly analysis.
    </p>

    <h3>üß† Multiple-Choice QA</h3>
    <ul>
      <li><strong>Reasoning hurts without structure:</strong> Base models often underperform when forced to reason via <code>&lt;think&gt;</code> prompts, indicating unstructured reasoning introduces noise.</li>
      <li><strong>RFT consistently boosts performance:</strong> Across MSAD and UCF-Crime, RFT significantly improves both <code>Acc<sub>w/o think</sub></code> and <code>Acc<sub>w/think</sub></code>, while maintaining output structure.</li>
      <li><strong>Best overall QA accuracy:</strong> VAU-R1 (Qwen2.5-VL-3B+RFT) achieves <strong>88.33%</strong> QA accuracy on MSAD and <strong>92.03%</strong> on UCF-Crime, outperforming all baselines.</li>
    </ul>

    <h3>üß© Anomaly Analysis</h3>
    <ul>
      <li><strong>RFT improves reasoning depth:</strong> On MSAD, Qwen2.5-VL-3B+RFT achieves a total VAU-Eval score of <strong>33.38</strong>, outperforming SFT (15.96) and base models.</li>
      <li><strong>Balanced reasoning quality:</strong> RFT achieves notable gains in <em>classification accuracy</em> (CLS), <em>key matching</em> (KM), and <em>factual consistency</em> (FAC), providing coherent and factual explanations.</li>
      <li><strong>Generalization to unseen data:</strong> On UCF-Crime, RFT models maintain higher VAU-Eval scores despite domain shift, showing better reasoning robustness than SFT.</li>
    </ul>

    <table border="1" cellspacing="0" cellpadding="6">
      <thead>
        <tr>
          <th rowspan="2">Dataset</th>
          <th rowspan="2">Model</th>
          <th colspan="2" style="text-align: center;">QA Accuracy (%)</th>
          <th colspan="6" style="text-align: center;">VAU-Eval (0-10)</th>
        </tr>
        <tr>
          <th>Acc (w/o think)</th>
          <th>Acc (w/ think)</th>
          <th>CLS</th>
          <th>KM</th>
          <th>FLU</th>
          <th>INF</th>
          <th>FAC</th>
          <th>Total</th>
        </tr>
      </thead>
      <tbody>
        <!-- MSAD -->
        <tr><td rowspan="9">MSAD</td><td>InternVL2.5-2B</td><td>76.67</td><td>72.08</td><td>6.84</td><td>6.23</td><td>8.55</td><td>6.64</td><td>6.64</td><td>34.90</td></tr>
        <tr><td>Qwen2.5-VL-7B</td><td>84.58</td><td>83.33</td><td>6.75</td><td>6.41</td><td>9.27</td><td>7.74</td><td>6.92</td><td>37.08</td></tr>
        <tr><td>InternVL2.5-8B-MPO</td><td>82.50</td><td>84.17</td><td>6.83</td><td>6.33</td><td>8.32</td><td>6.37</td><td>6.86</td><td>34.72</td></tr>
        <tr><td>Qwen2-VL-2B</td><td>77.08</td><td>72.50</td><td>5.94</td><td>5.43</td><td>8.77</td><td>6.29</td><td>5.90</td><td>32.25</td></tr>
        <tr><td>+ <span style="color: teal;">SFT</span></td><td>82.92</td><td><strong>85.83</strong></td><td>6.04</td><td>5.43</td><td>8.89</td><td>6.55</td><td>5.93</td><td>32.84</td></tr>
        <tr><td><strong>+ <span style="color: rgb(255, 81, 0);">RFT</span></strong></td><td><strong>82.92 <span style="color:red;">‚Üë5.84</span></strong></td><td>83.75 <strong><span style="color:red;">‚Üë11.25</span></strong></td><td>6.05 <span style="color:red;">‚Üë</span></td><td>5.49 <span style="color:red;">‚Üë</span></td><td>8.89</td><td>6.50 <span style="color:red;">‚Üë</span></td><td>6.05 <span style="color:red;">‚Üë</span></td><td>32.98 <span style="color:red;">‚Üë</span></td></tr>
        <tr><td>Qwen2.5-VL-3B</td><td>85.83</td><td>82.50</td><td>5.77</td><td>5.24</td><td>9.02</td><td>6.74</td><td>5.70</td><td>32.47</td></tr>
        <tr><td>+ <span style="color: teal;">SFT</span></td><td>86.25</td><td>84.58</td><td>2.89</td><td>2.22</td><td>4.89</td><td>3.52</td><td>2.44</td><td>15.96</td></tr>
        <tr><td><strong>+ <span style="color: rgb(255, 81, 0);">RFT</span></strong></td><td><strong>88.33 <span style="color:red;">‚Üë2.50</span></strong></td><td><strong>87.08 <span style="color:red;">‚Üë4.58</span></strong></td><td>5.97 <span style="color:red;">‚Üë</span></td><td>5.49 <span style="color:red;">‚Üë</span></td><td>9.05 <span style="color:red;">‚Üë</span></td><td>6.84 <span style="color:red;">‚Üë</span></td><td>6.03 <span style="color:red;">‚Üë</span></td><td>33.38 <span style="color:red;">‚Üë</span></td></tr>

        <!-- UCF-Crime -->
        <tr><td rowspan="9">UCF-Crime</td><td>InternVL2.5-2B</td><td>84.86</td><td>68.13</td><td>4.40</td><td>3.08</td><td>8.09</td><td>5.69</td><td>3.47</td><td>24.74</td></tr>
        <tr><td>Qwen2.5-VL-7B</td><td>92.03</td><td>89.64</td><td>4.80</td><td>3.73</td><td>8.95</td><td>7.05</td><td>4.25</td><td>28.78</td></tr>
        <tr><td>InternVL2.5-8B-MPO</td><td>89.64</td><td>90.44</td><td>3.79</td><td>3.20</td><td>8.23</td><td>5.77</td><td>3.48</td><td>24.47</td></tr>
        <tr><td>Qwen2-VL-2B</td><td>87.25</td><td>83.67</td><td>3.47</td><td>2.48</td><td>7.75</td><td>4.49</td><td>2.82</td><td>21.02</td></tr>
        <tr><td>+ <span style="color: teal;">SFT</span></td><td>83.67</td><td>86.06</td><td>3.61</td><td>2.26</td><td>7.30</td><td>4.79</td><td>2.70</td><td>20.66</td></tr>
        <tr><td><strong>+ <span style="color: rgb(255, 81, 0);">RFT</span></strong></td><td><strong>88.45 <span style="color:red;">‚Üë1.20</span></strong></td><td><strong>88.05 <span style="color:red;">‚Üë4.38</span></strong></td><td>4.04 <span style="color:red;">‚Üë</span></td><td>2.75 <span style="color:red;">‚Üë</span></td><td>7.72 <span style="color:blue;">‚Üì</span></td><td>4.89 <span style="color:red;">‚Üë</span></td><td>3.11 <span style="color:red;">‚Üë</span></td><td>22.52 <span style="color:red;">‚Üë</span></td></tr>
        <tr><td>Qwen2.5-VL-3B</td><td>91.63</td><td>83.27</td><td>4.31</td><td>2.88</td><td>8.70</td><td>5.95</td><td>3.27</td><td>25.10</td></tr>
        <tr><td>+ <span style="color: teal;">SFT</span></td><td>90.84</td><td>90.44</td><td>1.80</td><td>1.01</td><td>4.15</td><td>2.82</td><td>1.11</td><td>10.89</td></tr>
        <tr><td><strong>+ <span style="color: rgb(255, 81, 0);">RFT</span></strong></td><td><strong>92.03 <span style="color:red;">‚Üë0.40</span></strong></td><td><strong>91.63 <span style="color:red;">‚Üë8.36</span></strong></td><td>4.42 <span style="color:red;">‚Üë</span></td><td>2.98 <span style="color:red;">‚Üë</span></td><td>8.71 <span style="color:red;">‚Üë</span></td><td>5.98 <span style="color:red;">‚Üë</span></td><td>3.39 <span style="color:red;">‚Üë</span></td><td>25.49 <span style="color:red;">‚Üë</span></td></tr>
        <tr></tr>
      </tbody>
    </table>

    <center>
      <p style="font-size: medium; text-align: center; margin-top: 10px;">
        <i>Comparison of performance on MSAD and UCF-Crime datasets on multiple-choice QA task and anomaly analysis task.
        Results are reported for inference with and without Chain-of-Thought ("think") prompts.</i>
      </p>
    </center>

    <h3>üìç Temporal Anomaly Grounding</h3>
    <ul>
      <li><strong>RFT consistently outperforms</strong> base models in both inference modes, improving temporal localization and generalization. Notably, the RFT-finetuned 3B model achieves higher mIoU than the larger 7B base model on ECVA.</li>
      <li><strong>Chain-of-Thought prompting</strong> does not always help grounding; in some cases, it slightly degrades performance.</li>
      <li><strong>RFT generalizes better</strong> to unseen data. While SFT can occasionally match RFT, its outputs are often repetitive and less interpretable.</li>
    </ul>
    <table border="1" cellspacing="0" cellpadding="6">
      <thead>
        <tr>
          <th rowspan="2">Dataset</th>
          <th rowspan="2">Model</th>
          <th colspan="4">w/o think</th>
          <th colspan="4">w/ think</th>
        </tr>
        <tr>
          <th>mIoU</th>
          <th>R@0.3</th>
          <th>R@0.5</th>
          <th>R@0.7</th>
          <th>mIoU</th>
          <th>R@0.3</th>
          <th>R@0.5</th>
          <th>R@0.7</th>
        </tr>
      </thead>
      <tbody>
        <!-- MSAD -->
        <tr><td rowspan="5">MSAD</td><td>Qwen2-VL-2B</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr>
        <tr><td>Qwen2.5-VL-7B</td><td>45.90</td><td>70.83</td><td>45.83</td><td>21.67</td><td>17.57</td><td>26.67</td><td>11.67</td><td>3.33</td></tr>
        <tr><td>Qwen2.5-VL-3B</td><td>21.27</td><td>30.00</td><td>10.83</td><td>4.17</td><td>13.00</td><td>16.67</td><td>5.83</td><td>1.67</td></tr>
        <tr><td>+ <span style="color: teal;">SFT</span></td><td>30.65</td><td>47.50</td><td>30.00</td><td>9.17</td><td>35.17</td><td>50.83</td><td>34.17</td><td>15.00</td></tr>
        <tr><td><strong>+ <span style="color: rgb(255, 81, 0);">RFT</span></strong></td><td><strong>35.77 <span style="color: red;">‚Üë14.50</span></strong></td><td>53.33</td><td>34.17</td><td>15.83</td><td><strong>30.70 <span style="color: red;">‚Üë17.70</span></strong></td><td>48.33</td><td>29.17</td><td>12.50</td></tr>

        <!-- ECVA -->
        <tr><td rowspan="5">ECVA</td><td>Qwen2-VL-2B</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.17</td><td>0.30</td><td>0.00</td><td>0.00</td></tr>
        <tr><td>Qwen2.5-VL-7B</td><td>19.85</td><td>25.87</td><td>15.17</td><td>9.70</td><td>5.71</td><td>7.96</td><td>4.73</td><td>2.99</td></tr>
        <tr><td>Qwen2.5-VL-3B</td><td>14.21</td><td>17.16</td><td>6.47</td><td>3.23</td><td>6.35</td><td>7.21</td><td>1.99</td><td>0.50</td></tr>
        <tr><td>+ <span style="color: teal;">SFT</span></td><td>45.30</td><td>66.67</td><td>49.75</td><td>24.13</td><td>45.96</td><td>65.67</td><td>51.00</td><td>26.12</td></tr>
        <tr><td><strong>+ <span style="color: rgb(255, 81, 0);">RFT</span></strong></td><td><strong>35.09 <span style="color: red;">‚Üë20.88</span></strong></td><td>49.00</td><td>28.86</td><td>19.40</td><td><strong>33.25 <span style="color: red;">‚Üë26.90</span></strong></td><td>48.51</td><td>30.60</td><td>18.41</td></tr>

        <!-- UCF-Crime -->
        <tr style="background-color: #fffbe6;"><td rowspan="5">UCF-Crime (OOD)</td><td>Qwen2-VL-2B</td><td>2.74</td><td>4.84</td><td>0.00</td><td>0.00</td><td>0.12</td><td>0.00</td><td>0.00</td><td>0.00</td></tr>
        <tr style="background-color: #fffbe6;"><td>Qwen2.5-VL-7B</td><td>22.72</td><td>33.87</td><td>16.13</td><td>8.06</td><td>4.89</td><td>8.06</td><td>1.61</td><td>0.00</td></tr>
        <tr style="background-color: #fffbe6;"><td>Qwen2.5-VL-3B</td><td>10.91</td><td>15.32</td><td>6.45</td><td>3.23</td><td>7.68</td><td>10.48</td><td>4.84</td><td>1.61</td></tr>
        <tr style="background-color: #fffbe6;"><td>+ <span style="color: teal;">SFT</span></td><td>4.98</td><td>3.23</td><td>0.81</td><td>0.00</td><td>5.76</td><td>5.65</td><td>0.81</td><td>0.81</td></tr>
        <tr style="background-color: #fffbe6;"><td><strong>+ <span style="color: rgb(255, 81, 0);">RFT</span></strong></td><td><strong>16.80 <span style="color: red;">‚Üë5.89</span></strong></td><td>23.39</td><td>8.06</td><td>4.03</td><td><strong>9.21 <span style="color: red;">‚Üë1.53</span></strong></td><td>9.68</td><td>4.03</td><td>1.61</td></tr>
        <tr></tr>
      </tbody>
    </table>
    <p style="font-size: medium; text-align: center; margin-top: 10px;">
      <i>Comparison of temporal anomaly grounding performance on the three datasets. we evaluate temporal anomaly grounding on three datasets: <strong>MSAD</strong>, <strong>ECVA</strong>, and <strong>UCF-Crime</strong>. All models are trained only on MSAD and ECVA, while UCF-Crime is treated as an <em>out-of-distribution</em> (OOD) test set to assess cross-dataset generalization.</i>
    </p>


    <h3>üß™ Task Co-Training for Anomaly Classification</h3>
    <ul>
      <li><strong>TAG boosts perception</strong>: RFT with temporal grounding alone yields the best binary (74.14) and strong multi-class (46.14) accuracy under <code>w/ think</code>, underscoring the importance of temporal context.</li>
      <li><strong>QA + TAG are synergistic</strong>: Combining QA and TAG improves performance, though TAG alone remains most effective.</li>
      <li><strong>SFT tends to overfit</strong>: Despite high binary accuracy (83.37), SFT underperforms in multi-class classification, indicating reduced precision.</li>
      <li><strong>Multi-task RFT balances trade-offs</strong>: Jointly training with QA, TAG, and CLS via RFT leads to well-rounded reasoning and accuracy.</li>
    </ul>

    <table border="1" cellspacing="0" cellpadding="6">
      <thead>
        <tr>
          <th rowspan="2">Model</th>
          <th colspan="2" style="text-align: center;">w/o think</th>
          <th colspan="2" style="text-align: center;">w/ think</th>
        </tr>
        <tr>
          <th>Bin. Acc.</th>
          <th>Multi Acc.</th>
          <th>Bin. Acc.</th>
          <th>Multi Acc.</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Baseline (Qwen2.5-VL-3B-Instruct)</td>
          <td>62.77</td>
          <td>47.96</td>
          <td>59.33</td>
          <td>39.06</td>
        </tr>
        <tr>
          <td>+ <span style="color: teal;">SFT</span> w/ CLS</td>
          <td><strong>81.12</strong></td>
          <td>29.08</td>
          <td><strong>83.37</strong></td>
          <td>32.19</td>
        </tr>
        <tr>
          <td>+ <span style="color: rgb(255, 81, 0);">RFT</span> w/ CLS</td>
          <td>60.30</td>
          <td>46.14</td>
          <td>59.01</td>
          <td>42.27</td>
        </tr>
        <tr>
          <td>+ <span style="color: rgb(255, 81, 0);">RFT</span> w/ QA</td>
          <td>59.01</td>
          <td>46.14</td>
          <td>58.91</td>
          <td>41.95</td>
        </tr>
        <tr>
          <td>+ <span style="color: rgb(255, 81, 0);">RFT</span> w/ TAG</td>
          <td><u>67.81</u></td>
          <td><strong>49.46</strong></td>
          <td><u>74.14</u></td>
          <td><strong>46.14</strong></td>
        </tr>
        <tr>
          <td>+ <span style="color: rgb(255, 81, 0);">RFT</span> w/ QA-TAG</td>
          <td>65.77</td>
          <td>47.53</td>
          <td>67.60</td>
          <td>45.06</td>
        </tr>
        <tr>
          <td>+ <span style="color: rgb(255, 81, 0);">RFT</span> w/ QA-TAG-CLS</td>
          <td>64.70</td>
          <td><u>48.61</u></td>
          <td>65.02</td>
          <td><u>45.60</u></td>
        </tr>
      </tbody>
    </table>
    <p style="font-size: medium; text-align: center; margin-top: 10px;">
      <i>
        Ablation study of task co-training for anomaly classification. <strong>Bin. Acc.</strong> = binary accuracy (normal vs. abnormal); <strong>Multi Acc.</strong> = multi-class accuracy across 19 anomaly types plus the normal class.
      </i>
    </p>




    <h3>üîë Key Insights</h3>
    <ul>
      <li><strong>RFT outperforms SFT</strong>: Reinforcement Fine-Tuning leads to better reasoning accuracy and stronger generalization compared to standard supervised fine-tuning.</li>
      <li><strong>Interpretability through CoT</strong>: While Chain-of-Thought reasoning may not always boost raw visual understanding performance, it greatly enhances the interpretability of model outputs.</li>
      <li><strong>Reward-aligned decomposition matters</strong>: Breaking down complex visual tasks into reward-aligned subtasks enables more stable and robust learning during training.</li>
    </ul>
  </div>
</section>

<section class="section" id="CaseStudy">
  <div class="container is-max-desktop content">
    <h2 class="title">‚úèÔ∏è Case Study</h2>

    <!-- Case Toggle Tags -->
    <div class="tags are-medium">
      <span class="tag is-link is-light toggle-button" data-target="qa">Multi-choice QA</span>
      <span class="tag is-link is-light toggle-button" data-target="tag">Temporal Anomaly Grounding</span>
      <span class="tag is-link is-light toggle-button" data-target="reasoning">Anomaly Analysis</span>
    </div>

    <!-- QA Case -->
    <div id="qa" class="example-content is-hidden">
      <figure class="image">
        <img src="./static/images/qa_case_study.png" alt="QA Case Study">
      </figure>
      <p style="font-size: medium; text-align: center; margin-top: -10px;">
        <i>
          <strong>Qualitative case of the QA task.</strong> The correct answer is highlighted in orange. RFT yields more precise, interpretable QA choices, while SFT's output is less informative.
        </i>
      </p>
    </div>

    <!-- TAG Case -->
    <div id="tag" class="example-content is-hidden">
      <figure class="image">
        <img src="./static/images/tag_case_study.png" alt="TAG Case Study">
      </figure>
      <p style="font-size: medium; text-align: center; margin-top: -10px;">
        <i>
          <strong>Qualitative case of the TAG task.</strong> The ground-truth is highlighted in orange. RFT yields more precise anomaly intervals, while SFT‚Äôs output is inaccurate.
        </i>
      </p>
    </div>

    <!-- Reasoning Case -->
    <div id="reasoning" class="example-content is-hidden">
      <figure class="image">
        <img src="./static/images/reasoning_case_study.png" alt="Anomaly Analysis Case Study">
      </figure>
      <p style="font-size: medium; text-align: center; margin-top: -10px;">
        <i>
          <strong>Qualitative case of the Anomaly Analysis task.</strong> Correct descriptions and analyses are highlighted in orange. VAU-R1 identifies the anomaly with high fluency, though omits reasoning for the core event. SFT's output is less accurate and tends to repeat.
        </i>
      </p>
    </div>
  </div>
</section>





<section class="section" id="DatasetExamples">
  <div class="container is-max-desktop content">
    <h2 class="title">üìö Dataset Examples</h2>

    <!-- Radio-style Toggle Tags -->
    <div class="tags are-medium">
      <span class="tag is-link is-light toggle-button" data-target="example1">Explosion</span>
      <span class="tag is-link is-light toggle-button" data-target="example2">Stealing</span>
      <span class="tag is-link is-light toggle-button" data-target="example3">Normal</span>
    </div>

    <!-- Example Content Blocks -->
    <div id="example1" class="example-content is-hidden">
      <figure class="image">
        <img src="./static/images/example_1.png" alt="Dataset Example 1">
      </figure>
      <p style="font-size: medium; text-align: center; margin-top: -20px;">
        <i>
          An <strong>explosion</strong> case in an outdoor backyard, highlighting complex anomaly detection and dynamic scene understanding. The clip is labeled with a question-answer pair, key visual evidence, anomaly type, and a multi-part reasoning chain covering location, cause-effect, and a high-level conclusion.
        </i>
      </p>
    </div>

    <div id="example2" class="example-content is-hidden">
      <figure class="image">
        <img src="./static/images/example_3.png" alt="Dataset Example 2">
      </figure>
      <p style="font-size: medium; text-align: center; margin-top: -20px;">
        <i>
          An example of a <strong>stealing</strong> incident, demonstrating capabilities in human activity recognition and intent analysis.
        </i>
      </p>
    </div>

    <div id="example3" class="example-content is-hidden">
      <figure class="image">
        <img src="./static/images/example_2.png" alt="Dataset Example 3">
      </figure>
      <p style="font-size: medium; text-align: center; margin-top: -20px;">
        <i>
          A <strong>normal</strong> scene, used to evaluate model robustness against false positives and to enhance dataset diversity.
        </i>
      </p>
    </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhu2025vaur1,
      title={VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning}, 
      author={Liyun Zhu and Qixiang Chen and Xi Shen and Xiaodong Cun},
      year={2025},
      eprint={2505.23504},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.23504}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="content">
        <p style="font-size:smaller;">
          This page was built upon the template provided by 
          <a href="https://nerfies.github.io/">Nerfies</a> project page.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
